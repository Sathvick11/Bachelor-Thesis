
@misc{moo,
  author         = {Garai Balint},
  howpublished            = {\url{https://inside-docupedia.bosch.com/confluence/display/MOO/Introduction}},
  title          = {Introduction to MOO},
  note           = {Online; Accessed on 27/05/2024}
}

@misc{optislang,
  author         = {},
  title          = "{Ansys Optislang}",
  howpublished = {\url{https://www.ansys.com/products/connect/ansys-optislang}},
  note           = {Online; Accessed on 10/05/2024}
}

@misc{python,
  author = {},
  howpublished    = {\url{https://www.python.org/}},
  title  = {Python},
  note   = {Online; Accessed on 07/08/2024}
}

@misc{devops_lifecycle,
	howpublished = {\url{https://www.openxcell.com/devops}},
	title = "{DevOps} - the complete guide for 2023",
	note = {Online; Accessed on 15/08/2024}
}

@misc{framework,
	howpublished = {\url{https://www.geeksforgeeks.org/what-is-a-framework/}},
	title = "{What is a Framework?}",
	note = {Online; Accessed on 13/09/2024}
}

@inproceedings{8257807,
  author    = {Perera, Pulasthi and Silva, Roshali and Perera, Indika},
  booktitle = {2017 Seventeenth International Conference on Advances in ICT for Emerging Regions (ICTer)},
  title     = {Improve software quality through practicing DevOps},
  year      = {2017},
  volume    = {},
  number    = {},
  pages     = {1-6},
  abstract  = {DevOps is extended from certain agile practices with a mix of patterns intended to improve collaboration between development and operation teams. The main purpose of this paper is to conduct a study on how DevOps practice has impacted to software quality. The secondary objective is to find how to improve quality efficiently. A literature survey has carried out to explore about current DevOps practices in industry. According to the literature survey, the conceptual research model was developed and five hypotheses were derived. Research objectives were accomplished by testing hypotheses using Pearson correlation. A linear model is derived based on the linear regression analysis. An online questionnaire was used to collect quantitative data whereas interviews with experts on DevOps and Quality assurance have been used to identify how to improve the quality of software by practicing DevOps. Recommendations are given based on interview feedback, hypotheses testing with regression analysis. According to the quantitative study, researchers have identified that quality of the software gets improved when practice DevOps by following CAMS (Culture, Automation, Measurement, Sharing) framework. Automation is the most critical factor to improve the software quality. As per the results of multiple regression analysis, it has proved culture, automation, measurement and sharing are important factors to consider to improve quality of the software. In conclusion it can be recommended to use DevOps to achieve high quality software.},
  keywords  = {Companies;Automation;Software quality;Testing;Software measurement;DevOps;CAMS Framework;Quality;ISO 9126;Automation},
  DOI       = {10.1109/ICTER.2017.8257807},
  issn      = {2472-7598},
  month     = {Sep.}
}

@inproceedings{10616918,
  author    = {Naresh, E and Murthy, S V N and Sreenivasa, N. and Merikapudi, Seshaiah and Rakhi Krishna, C R},
  booktitle = {2024 International Conference on Knowledge Engineering and Communication Systems (ICKECS)},
  title     = {Continuous Integration, Testing Deployment and Delivery in Devops},
  year      = {2024},
  volume    = {1},
  number    = {},
  pages     = {1-4},
  abstract  = {The proposed research article focuses on Continuous Integration, Testing, Delivery and Deployment in terms of DevOps environment. DevOps is a combination of development and operations. In a usual work environment, both development and operations of that company work entirely differently where mutual interaction is said to be minimum. Thus, to overcome that, DevOps was introduced, where both development and operations must work together in the same environment. In this article we will discuss what each of these terms means, how they work, why they are important in todayâ€™s software development life cycle (SDLC), what tools are used for each of them, etc. We will consider a few scenarios where the above-mentioned topics are better than the traditional or the most widely used SDLC methodologies.},
  keywords  = {Knowledge engineering;DevOps;Computer bugs;Companies;Production;Continuous integration;Software;Continuous Integration (CI);Continuous Testing (CT);Continuous Development (CD);Continuous Delivery (CD);DevOps;Software development Life Cycle (SDLC)},
  DOI       = {10.1109/ICKECS61492.2024.10616918},
  issn      = {},
  month     = {April}
}

@ARTICLE{6802994,
  author={Meyer, Mathias},
  journal={IEEE Software}, 
  title={Continuous Integration and Its Tools}, 
  year={2014},
  volume={31},
  number={3},
  pages={14-16},
  abstract={Continuous integration has been around for a while now, but the habits it suggests are far from common practice. Automated builds, a thorough test suite, and committing to the mainline branch every day sound simple at first, but they require a responsible team to implement and constant care. What starts with improved tooling can be a catalyst for long-lasting change in your company's shipping culture. Continuous integration is more than a set of practices, it's a mindset that has one thing in mind: increasing customer value. The Web extra at http://youtu.be/tDl_cHfrJZo is an audio podcast of the Tools of the Trade column discusses how continuous integration is more than a set of practices, it's a mindset that has one thing in mind: increasing customer value.},
  keywords={Production;Servers;Monitoring;Software;Green products;Marine vehicles;Multimedia communication;continuous integration;continuous delivery;testing},
  DOI={10.1109/MS.2014.58},
  ISSN={1937-4194},
  month={May},}

@misc{framework,
  author = {},
  howpublished    = {\url{https://www.geeksforgeeks.org/what-is-a-framework/}},
  title  = {What is a Framework?},
  note   = {Online; Accessed 03/09/2024}
}

@INPROCEEDINGS{8421965,
  author={Arachchi, S.A.I.B.S. and Perera, Indika},
  booktitle={2018 Moratuwa Engineering Research Conference (MERCon)}, 
  title={Continuous Integration and Continuous Delivery Pipeline Automation for Agile Software Project Management}, 
  year={2018},
  volume={},
  number={},
  pages={156-161},
  abstract={Agile practices with Continuous Integration and Continuous Delivery (CICD) pipeline approach has increased the efficiency of projects. In agile, new features are introduced to the system in each sprint delivery, and although it may be well developed, the delivery failures are possible due to performance issues. By considering delivery timeline, moving for system scaling is common solution in such situations. But, how much system should be scaled? System scale requires current system benchmark status and expected system status. Benchmarking the production is a critical task, as it interrupts the live system. The new version should go through a load test to measure expected system status. The traditional load test methods are unable to identify production performance behavior due to simulated traffic patterns are highly deviated from production. To overcome those issues, this approach has extended CICD pipeline to have three automation phases named benchmark, load test and scaling. It minimizes the system interruption by using test bench approach when system benchmarking and it uses the production traffic for load testing which gives more accurate results. Once benchmark and load test phases are completed, system scaling can be evaluated. Initially, the pipeline was developed using Jenkins CI server, Git repository and Nexus repository with Ansible automation. Then GoReplay is used for traffic duplication from production to test bench environment. Nagios monitoring is used to analyze the system behavior in each phase and the result of test bench has proven that scaling is capable to handle the same load while changing the application software, but it doesn't optimize response time of application at significant level and it helps to reduce the risk of application deployment by integrating this three phase approach as CICD automation extended feature. Thereby the research provides effective way to manage Agile based CICD projects.},
  keywords={Production;Benchmark testing;Software;Tools;Automation;Pipelines;continuous integration;continuous delivery;agile project management;version management;configuration management},
  doi={10.1109/MERCon.2018.8421965},
  ISSN={},
  month={May},}
@ARTICLE{6862882,
author={Athanasiou, Dimitrios and Nugroho, Ariadi and Visser, Joost and Zaidman, Andy},
journal={IEEE Transactions on Software Engineering}, 
title={Test Code Quality and Its Relation to Issue Handling Performance}, 
year={2014},
volume={40},
number={11},
pages={1100-1125},
abstract={Automated testing is a basic principle of agile development. Its benefits include early defect detection, defect causelocalization and removal of fear to apply changes to the code. Therefore, maintaining high quality test code is essential. This study introduces a model that assesses test code quality by combining source code metrics that reflect three main aspects of test codequality: completeness, effectiveness and maintainability. The model is inspired by the Software Quality Model of the SoftwareImprovement Group which aggregates source code metrics into quality ratings based on benchmarking. To validate the model we assess the relation between test code quality, as measured by the model, and issue handling performance. An experiment isconducted in which the test code quality model is applied to $18$  open source systems. The test quality ratings are tested for correlation with issue handling indicators, which are obtained by mining issue repositories. In particular, we study the (1) defect resolution speed, (2) throughput and (3) productivity issue handling metrics. The results reveal a significant positive correlation between test code quality and two out of the three issue handling metrics (throughput and productivity), indicating that good test code quality positively influences issue handling performance.},
keywords={Measurement;Software;Productivity;Throughput;Benchmark testing;Correlation;Testing;defects;bugs;metrics;measurement},
doi={10.1109/TSE.2014.2342227},
ISSN={1939-3520},
month={Nov},}

@INPROCEEDINGS{10434454,
  author={Nithin, S. and S J, Hamsalekha and S, Parvathy},
  booktitle={2023 Innovations in Power and Advanced Computing Technologies (i-PACT)}, 
  title={Enhancing the Automotive Software Test Environment Using Continuous Integration and Validation Pipeline}, 
  year={2023},
  volume={},
  number={},
  pages={1-6},
  abstract={Software testing is an integral part of the software development lifecycle, ensuring that software meets quality and functionality requirements. Traditionally, testing has been carried out at specific stages, such as during development or before release. However, this approach often leads to delayed defect detection, increased costs, and reduced overall software quality. To overcome these challenges, the concept of continuous testing has emerged as a critical practice in software testing. Continuous testing (CT) is an iterative and automated process that integrates testing activities throughout the software development cycle. It involves running tests early, frequently, and consistently, enabling immediate feedback on code changes and rapid identification of defects. Continental Automotive introduces a novel solution to address specific project requirements that cannot be fulfilled by Jenkins Due to project dependencies. Introducing the Contest Tool, an internally developed application designed for seamless Continuous Testing. This powerful tool enables downloading of project-specific source application files, seamlessly loading the software to the ECU, conducting comprehensive smoke tests, and generating detailed reports. Moreover, the Contest Tool possesses the unique capability of identifying variances in project-specific checkpoints, allowing for efficient Continuous Testing even when differences arise in the source application files. With its comprehensive features and adaptability, the Contest Tool revolutionizes the testing process, ensuring enhanced quality and reliability in project execution.},
  keywords={Software testing;Technological innovation;Loading;Software;Standards;Testing;Automotive engineering;Electronic Control Unit;Continuous Testing;Continuous Integration Continuous Development},
  doi={10.1109/i-PACT58649.2023.10434454},
  ISSN={},
  month={Dec},}

@INPROCEEDINGS{9463074,
  author={Kinsman, Timothy and Wessel, Mairieli and Gerosa, Marco A. and Treude, Christoph},
  booktitle={2021 IEEE/ACM 18th International Conference on Mining Software Repositories (MSR)}, 
  title={How Do Software Developers Use GitHub Actions to Automate Their Workflows?}, 
  year={2021},
  volume={},
  number={},
  pages={420-431},
  abstract={Automated tools are frequently used in social coding repositories to perform repetitive activities that are part of the distributed software development process. Recently, GitHub introduced GitHub Actions, a feature providing automated work-flows for repository maintainers. Although several Actions have been built and used by practitioners, relatively little has been done to evaluate them. Understanding and anticipating the effects of adopting such kind of technology is important for planning and management. Our research is the first to investigate how developers use Actions and how several activity indicators change after their adoption. Our results indicate that, although only a small subset of repositories adopted GitHub Actions to date, there is a positive perception of the technology. Our findings also indicate that the adoption of GitHub Actions increases the number of monthly rejected pull requests and decreases the monthly number of commits on merged pull requests. These results are especially relevant for practitioners to understand and prevent undesirable effects on their projects.},
  keywords={Tools;Encoding;Data models;Planning;Data mining;IEEE activities;Open source software;GitHub Actions;GitHub Bots;Automated work-flow;Regression Discontinuity Design},
  doi={10.1109/MSR52588.2021.00054},
  ISSN={2574-3864},
  month={May},}

@misc{fowler2006continuous,
  title="{Continuous Integration}",
  author={Fowler, Martin and Foemmel, Matthew},
  year={2006},
}

@misc{github_actions,
  author = {},
  howpublished    = {\url{https://docs.github.com/en/actions}},
  title  = "{GitHub Actions}",
  note   = {Online; Accessed on 25/09/2024}
}

@INPROCEEDINGS{8444829,
  author={Larios Vargas, Enrique and Hejderup, Joseph and Kechagia, Maria and Bruntink, Magiel and Gousios, Georgios},
  booktitle={2018 IEEE/ACM 40th International Conference on Software Engineering: New Ideas and Emerging Technologies Results (ICSE-NIER)}, 
  title={Enabling Real-Time Feedback in Software Engineering}, 
  year={2018},
  volume={},
  number={},
  pages={21-24},
  abstract={Modern software projects consist of more than just code: teams follow development processes, the code runs on servers or mobile phones and produces run time logs and users talk about the software in forums like StackOverflow and Twitter and rate it on app stores. Insights stemming from the real-time analysis of combined software engineering data can help software practitioners to conduct faster decision-making. With the development of CodeFeedr, a Real-time Software Analytics Platform, we aim to make software analytics a core feedback loop for software engineering projects. CodeFeedr's vision entails: (1) The ability to unify archival and current software analytics data under a single query language, and (2) The feasibility to apply new techniques and methods for high-level aggregation and summarization of near real-time information on software development. In this paper, we outline three use cases where our platform is expected to have a significant impact on the quality and speed of decision making; dependency management, productivity analytics, and run-time error feedback.},
  keywords={Software;Real-time systems;Computer bugs;Productivity;Libraries;Software engineering;software analytics;real time feedback;streaming data},
  doi={},
  ISSN={},
  month={May},}

@article{non_linear_optimization,
author = {Bucher, Christian and Will, Johannes and Riedel, JÃ¶rg},
pages = {},
title = "{Multidisciplinary non-linear optimization with Optimizing Structural Language OptiSLang}"
}

@article{sbpipe,
author = {Pezze, Piero and Gambardella, Nicolas},
year = {2017},
month = {12},
pages = {},
title = {SBpipe: A collection of pipelines for automating repetitive simulation and analysis tasks},
volume = {11},
journal = {BMC Systems Biology},
doi = {10.1186/s12918-017-0423-3}
}

@INPROCEEDINGS{6200132,
  author={Jolly, Shahnewaz Amin and Garousi, Vahid and Eskandar, Matt M.},
  booktitle={2012 IEEE Fifth International Conference on Software Testing, Verification and Validation}, 
  title={Automated Unit Testing of a SCADA Control Software: An Industrial Case Study Based on Action Research}, 
  year={2012},
  volume={},
  number={},
  pages={400-409},
  abstract={We report in this case-study paper our experience and success story with a practical approach and tool for unit regression testing of a SCADA (Supervisory Control and Data Acquisition) software. The tool uses a black-box specification of the units under test to automatically generate NUnit test code. We then improved the test suite by white-box and mutation testing. The approach and tool were developed in an action-research project to test a commercial large-scale SCADA system called Rocket.},
  keywords={Automation;Software;Engines;Software testing;Context;Manuals;automated unit testing;SCADA software;industrical case study;action research},
  doi={10.1109/ICST.2012.120},
  ISSN={2159-4848},
  month={April},}
  
@INPROCEEDINGS{6465286,
	author={Perrone, L. Felipe and Main, Christopher S. and Ward, Bryan C.},
	booktitle={Proceedings of the 2012 Winter Simulation Conference (WSC)}, 
	title={SAFE: Simulation automation framework for experiments}, 
	year={2012},
	volume={},
	number={},
	pages={1-12},
	abstract={The workflow of a network simulation study requires adherence to best practices in methodology so that results are credible and reproducible by third parties. The opportunities for one to introduce errors start at model description and permeate the process through to the reporting of results. The literature indicates that even publications in respected venues include inadvertent mistakes and poor application of methodology. When experts are liable to fail, it is unreasonable to expect that students would fare any better. This paper presents a system designed to provide guidance for inexperienced users of the popular ns-3 network simulator. SAFE automates the workflow from the initialization of model parameters, to the parallelized execution of experiments, to the processing and persistent storage of output data, and to graphical visualization of results. We discuss the architecture and the implementation of the system in the context of similar contributions in the literature.},
	keywords={Computational modeling;Data models;Databases;Analytical models;Data visualization;Communities},
	doi={10.1109/WSC.2012.6465286},
	ISSN={1558-4305},
	month={Dec},}

@ARTICLE{553698,
	author={Stocks, P. and Carrington, D.},
	journal={IEEE Transactions on Software Engineering}, 
	title={A framework for specification-based testing}, 
	year={1996},
	volume={22},
	number={11},
	pages={777-793},
	abstract={Test templates and a test template framework are introduced as useful concepts in specification-based testing. The framework can be defined using any model-based specification notation and used to derive tests from model-based specifications-in this paper, it is demonstrated using the Z notation. The framework formally defines test data sets and their relation to the operations in a specification and to other test data sets, providing structure to the testing process. Flexibility is preserved, so that many testing strategies can be used. Important application areas of the framework are discussed, including refinement of test data, regression testing, and test oracles.},
	keywords={Software testing;Application software;Formal specifications;Computer science;Life testing;Object oriented modeling;Computer Society;Programming;Performance evaluation;Software design},
	doi={10.1109/32.553698},
	ISSN={1939-3520},
	month={Nov},}

@INPROCEEDINGS{6319254,
	author={Vos, Tanja E.J. and MarÃ­n, Beatriz and Escalona, Maria Jose and Marchetto, Alessandro},
	booktitle={2012 12th International Conference on Quality Software}, 
	title={A Methodological Framework for Evaluating Software Testing Techniques and Tools}, 
	year={2012},
	volume={},
	number={},
	pages={230-239},
	abstract={There exists a real need in industry to have guidelines on what testing techniques use for different testing objectives, and how usable (effective, efficient, satisfactory) these techniques are. Up to date, these guidelines do not exist. Such guidelines could be obtained by doing secondary studies on a body of evidence consisting of case studies evaluating and comparing testing techniques and tools. However, such a body of evidence is also lacking. In this paper, we will make a first step towards creating such body of evidence by defining a general methodological evaluation framework that can simplify the design of case studies for comparing software testing tools, and make the results more precise, reliable, and easy to compare. Using this framework, (1) software testing practitioners can more easily define case studies through an instantiation of the framework, (2) results can be better compared since they are all executed according to a similar design, (3) the gap in existing work on methodological evaluation frameworks will be narrowed, and (4) a body of evidence will be initiated. By means of validating the framework, we will present successful applications of this methodological framework to various case studies for evaluating testing tools in an industrial environment with real objects and real subjects.},
	keywords={Software;Software testing;Companies;Guidelines;Software engineering;Taxonomy;Case study;Software testing techniques;Evaluation;Methodological framework},
	doi={10.1109/QSIC.2012.16},
	ISSN={2332-662X},
	month={Aug},}

@article{mathworks,
  author = {Khetarpal, Puneet and Dragic, Marco and Malleichervu, Govind},
  journal = {MathWorks},
  number = {},
  title = "{Accelerating Simulink Simulations in Continuous Integration Workflows with Simulink Cache Files}",
  volume = {},
  year = {2022}
}

@article{dspace,
  author = {},
  journal = {dSPACE},
  number = {},
  title = "{Successfully Applying Continuous Integration for HIL Simulation}",
  volume = {},
  year = {2021},
  month = {June}
}

@article{windriver,
  author = {},
  journal = {Windriver},
  number = {},
  title = "{Accelerate DevOps with Continuous Integration and Simulation}",
  volume = {},
  year = {2022},
  month = {May}
}

@Inbook{Benson2009,
author="Benson, Harold P.",
editor="Floudas, Christodoulos A.
and Pardalos, Panos M.",
title="Multi-objective optimization: pareto optimal solutions, propertiesMulti-objective Optimization: Pareto Optimal Solutions, Properties",
bookTitle="Encyclopedia of Optimization",
year="2009",
publisher="Springer US",
address="Boston, MA",
pages="2478--2481",
abstract="Keywords",
isbn="978-0-387-74759-0",
doi="10.1007/978-0-387-74759-0_426",
url="https://doi.org/10.1007/978-0-387-74759-0_426"
}

@misc{openshift,
  author = {},
  howpublished = {\url{https://www.redhat.com/en/technologies/cloud-computing/openshift}},
  title = "{Red Hat OpenShift}",
  note = {Online; Accessed on 15/09/2024}
}

@misc{docker,
  author = {},
  howpublished = {\url{https://www.docker.com/}},
  title = {Docker},
  note = {Online; Accessed on 15/09/2024}
}

@misc{fastapi,
  author = {},
  howpublished = {\url{https://fastapi.tiangolo.com/}},
  title = "{FastAPI}",
  note = {Online; Accessed on 20/09/2024}
}